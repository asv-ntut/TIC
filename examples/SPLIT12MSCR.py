import os
import shutil
import random
from pathlib import Path
import math
import concurrent.futures
import time

# ================= è¨­å®šå€ =================
SOURCE_DIR = r"D:\s2_cloudfree"

# è¨­å®šåˆ†å‰²æ¯”ä¾‹ (8:1:1)
TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1

# ã€å®‰å…¨åŠ é€Ÿè¨­å®šã€‘
# å»ºè­°è¨­å®šç‚º 4 æˆ– 8ã€‚
# è¨­å®šå¤ªé«˜ (å¦‚ 32) æœƒå°è‡´ç¡¬ç¢Ÿè®€å¯«é ­éç†±æˆ–ç³»çµ±å¡æ­»ã€‚
MAX_WORKERS = 4


# =========================================

def safe_move(file_info):
    """
    å–®å€‹æª”æ¡ˆç§»å‹•çš„å‡½æ•¸ï¼Œçµ¦åŸ·è¡Œç·’å‘¼å«ç”¨
    file_info: (source_path, target_dir)
    """
    src, target_dir = file_info
    try:
        # æ§‹å»ºç›®æ¨™è·¯å¾‘
        dst = target_dir / src.name
        shutil.move(str(src), str(dst))
        return True
    except Exception as e:
        # é‡åˆ°éŒ¯èª¤ (å¦‚æª”åé‡è¤‡) å˜—è©¦æ”¹å
        try:
            new_name = f"{src.stem}_dup_{int(time.time() * 1000)}{src.suffix}"
            dst = target_dir / new_name
            shutil.move(str(src), str(dst))
            return True
        except Exception as e2:
            return f"Error: {src.name} - {str(e2)}"


def flatten_and_split_dataset_parallel():
    source_path = Path(SOURCE_DIR)

    if not source_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è·¯å¾‘ {SOURCE_DIR}")
        return

    # 1. å»ºç«‹ç›®æ¨™è³‡æ–™å¤¾
    subsets = ['train', 'val', 'test']
    for subset in subsets:
        (source_path / subset).mkdir(exist_ok=True)

    print(f"ğŸ” æ­£åœ¨æœå°‹ {SOURCE_DIR} ä¸‹çš„æ‰€æœ‰ .tif æª”æ¡ˆ (é€™éœ€è¦ä¸€é»æ™‚é–“)...")

    # 2. æ”¶é›†æ‰€æœ‰æª”æ¡ˆ (éæ¿¾æ‰å·²ç¶“åœ¨ train/val/test çš„)
    all_tif_files = []
    for f in source_path.rglob("*.tif"):
        if f.parent.name not in subsets:
            all_tif_files.append(f)

    total_files = len(all_tif_files)
    print(f"ğŸ“¦ ç¸½å…±æ‰¾åˆ° {total_files} å€‹å¾…è™•ç†æª”æ¡ˆ")

    if total_files == 0:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°éœ€è¦ç§»å‹•çš„æª”æ¡ˆã€‚")
        return

    # 3. éš¨æ©Ÿæ‰“äº‚
    print("ğŸ² æ­£åœ¨æ‰“äº‚æ•¸æ“š...")
    random.seed(42)
    random.shuffle(all_tif_files)

    # 4. è¨ˆç®—åˆ†å‰²
    train_count = math.floor(total_files * TRAIN_RATIO)
    val_count = math.floor(total_files * VAL_RATIO)

    train_files = all_tif_files[:train_count]
    val_files = all_tif_files[train_count: train_count + val_count]
    test_files = all_tif_files[train_count + val_count:]

    print(f"ğŸ“Š åˆ†å‰²è¨ˆç•«: Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}")
    print(f"ğŸš€ å•Ÿå‹• {MAX_WORKERS} å€‹ä¸¦è¡Œä»»å‹™é–‹å§‹ç§»å‹• (ä¸æœƒå¡æ­»é›»è…¦)...")

    # 5. æº–å‚™ä»»å‹™æ¸…å–®
    # å°‡ (æª”æ¡ˆè·¯å¾‘, ç›®æ¨™è³‡æ–™å¤¾) æ‰“åŒ…æˆä¸€å€‹åˆ—è¡¨
    tasks = []
    tasks.extend([(f, source_path / 'train') for f in train_files])
    tasks.extend([(f, source_path / 'val') for f in val_files])
    tasks.extend([(f, source_path / 'test') for f in test_files])

    # 6. å¤šåŸ·è¡Œç·’åŸ·è¡Œ
    start_time = time.time()
    moved_count = 0
    total_tasks = len(tasks)

    # ä½¿ç”¨ ThreadPoolExecutor é€²è¡Œå¤šå·¥
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # submit æ‰€æœ‰ä»»å‹™
        futures = [executor.submit(safe_move, task) for task in tasks]

        # ç›£æ§é€²åº¦
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            result = future.result()
            moved_count += 1

            # æ¯æ¬ç§» 2000 å€‹æª”æ¡ˆæ‰å°ä¸€æ¬¡é€²åº¦ï¼Œæ¸›å°‘è¢å¹•è¼¸å‡ºé€ æˆçš„å»¶é²
            if moved_count % 2000 == 0:
                elapsed = time.time() - start_time
                speed = moved_count / elapsed
                print(f"   [é€²åº¦ {moved_count}/{total_tasks}] - é€Ÿåº¦: {speed:.0f} æª”/ç§’")

    end_time = time.time()
    duration = end_time - start_time
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼è€—æ™‚: {duration:.2f} ç§’")
    print(f"ğŸ“‚ è«‹æª¢æŸ¥ {SOURCE_DIR} ä¸‹çš„ train, val, test è³‡æ–™å¤¾ã€‚")
    print("ğŸ§¹ æç¤ºï¼šåŸæœ¬çš„ç©ºè³‡æ–™å¤¾ç¾åœ¨å¯ä»¥æ‰‹å‹•åˆªé™¤äº†ã€‚")


if __name__ == "__main__":
    # é€™è£¡åŠ ä¸€å€‹ä¿è­·ï¼Œé˜²æ­¢ Windows ä¸‹å¤šé€²ç¨‹å‡ºéŒ¯ (é›–ç„¶é€™è£¡æ˜¯å¤šç·šç¨‹ï¼Œä½†å¥½ç¿’æ…£)
    flatten_and_split_dataset_parallel()