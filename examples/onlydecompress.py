# import argparse
# import os
# import sys
# import glob
# import re
# import math
# from collections import OrderedDict
#
# import torch
# import torch.nn.functional as F
# from torchvision import transforms
# from PIL import Image
#
# # ==============================================================================
# # è¨­å®šï¼šåŒ¯å…¥æ¨¡å‹
# # ==============================================================================
# try:
#     from conv2 import SimpleConvStudentModel
# except ImportError:
#     print("éŒ¯èª¤: æ‰¾ä¸åˆ° conv2.pyï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚")
#     sys.exit(1)
#
# # å˜—è©¦åŒ¯å…¥ rasterio (è‹¥åŸå§‹åœ–æ˜¯ TIF éœ€è¦)
# try:
#     import rasterio
# except ImportError:
#     rasterio = None
#     # å˜—è©¦åŒ¯å…¥ ms_ssim (ç”¨æ–¼è¨ˆç®—åˆ†æ•¸)
# try:
#     from pytorch_msssim import ms_ssim
#
#     HAS_MSSSIM = True
# except ImportError:
#     print("è­¦å‘Š: æœªå®‰è£ pytorch_msssimï¼Œå°‡è·³é SSIM è¨ˆç®—ã€‚(å¯åŸ·è¡Œ pip install pytorch-msssim å®‰è£)")
#     HAS_MSSSIM = False
#
#
# # ==============================================================================
# # Monkey Patching: æ³¨å…¥è§£å£“ç¸®æ–¹æ³•
# # ==============================================================================
# def decompress_method(self, strings, shape):
#     assert isinstance(strings, list) and len(strings) == 2
#     z_hat = self.entropy_bottleneck.decompress(strings[1], shape)
#     gaussian_params = self.h_s(z_hat)
#     scales_hat, means_hat = gaussian_params.chunk(2, 1)
#     indexes = self.gaussian_conditional.build_indexes(scales_hat)
#     y_hat = self.gaussian_conditional.decompress(strings[0], indexes, means=means_hat)
#     x_hat = self.g_s(y_hat).clamp_(0, 1)
#     return {"x_hat": x_hat}
#
#
# SimpleConvStudentModel.decompress = decompress_method
#
#
# # ==============================================================================
# # å·¥å…·å‡½å¼
# # ==============================================================================
# def load_bin_file(bin_path):
#     """è®€å– .bin æª”æ¡ˆä¸¦é‚„åŸæˆ strings å’Œ shape"""
#     with open(bin_path, "rb") as f:
#         # è®€å– shape
#         h = int.from_bytes(f.read(2), 'little')
#         w = int.from_bytes(f.read(2), 'little')
#         shape = (h, w)
#
#         # è®€å– z_string
#         len_z = int.from_bytes(f.read(4), 'little')
#         z_str = f.read(len_z)
#
#         # è®€å– y_string
#         len_y = int.from_bytes(f.read(4), 'little')
#         y_str = f.read(len_y)
#
#     return {"strings": [[y_str], [z_str]], "shape": shape}
#
#
# @torch.no_grad()
# def process_decompress(model, bin_path, device):
#     data = load_bin_file(bin_path)
#     out_dec = model.decompress(data["strings"], data["shape"])
#     x_hat = out_dec["x_hat"]
#
#     # è™•ç† Patch å¤§å° (é è¨­ 256)
#     target_h, target_w = 256, 256
#     curr_h, curr_w = x_hat.size(2), x_hat.size(3)
#
#     if curr_h != target_h or curr_w != target_w:
#         padding_left = (curr_w - target_w) // 2
#         padding_top = (curr_h - target_h) // 2
#         x_hat = x_hat[:, :, padding_top:padding_top + target_h, padding_left:padding_left + target_w]
#
#     return x_hat
#
#
# def psnr(a: torch.Tensor, b: torch.Tensor) -> float:
#     """è¨ˆç®— PSNR"""
#     mse = F.mse_loss(a, b).item()
#     return -10 * math.log10(mse) if mse > 0 else float('inf')
#
#
# def read_original_image(filepath: str) -> torch.Tensor:
#     """è®€å–åŸå§‹åœ–ç‰‡ä¸¦è½‰ç‚º Tensor (èˆ‡ä½ åŸæœ¬çš„é‚è¼¯ä¸€è‡´)"""
#     ext = os.path.splitext(filepath)[-1].lower()
#     if ext in ['.tif', '.tiff']:
#         if rasterio is None: raise RuntimeError("éœ€å®‰è£ rasterio æ‰èƒ½è®€å– TIF")
#         SCALE = 10000.0
#         with rasterio.open(filepath) as src:
#             raw_data = src.read().astype(np.float32)
#         if np.isnan(raw_data).any(): raw_data = np.nan_to_num(raw_data)
#         rgb_data = raw_data[:3, :, :] if raw_data.shape[0] >= 3 else raw_data
#         clipped_data = np.clip(rgb_data, 0.0, 10000.0)
#         return torch.from_numpy(clipped_data / SCALE)
#     else:
#         img = Image.open(filepath).convert("RGB")
#         return transforms.ToTensor()(img)
#
#
# def load_checkpoint(checkpoint_path):
#     print(f"Loading checkpoint: {checkpoint_path}")
#     checkpoint = torch.load(checkpoint_path, map_location='cpu')
#     state_dict = checkpoint.get("state_dict", checkpoint)
#     new_state_dict = OrderedDict()
#     for k, v in state_dict.items():
#         name = k[7:] if k.startswith('module.') else k
#         new_state_dict[name] = v
#
#     N, M = 128, 192
#     try:
#         N = new_state_dict['g_a.0.weight'].size(0)
#         keys = sorted([k for k in new_state_dict.keys() if 'g_a' in k and 'weight' in k])
#         M = new_state_dict[keys[-1]].size(0)
#     except:
#         pass
#
#     model = SimpleConvStudentModel(N=N, M=M)
#     model.load_state_dict(new_state_dict, strict=False)
#     return model.eval()
#
#
# # ==============================================================================
# # ä¸»ç¨‹å¼
# # ==============================================================================
# def main():
#     parser = argparse.ArgumentParser(description="Image Decompression & PSNR Tool")
#     parser.add_argument("bin_dir", type=str, help="Directory containing .bin files")
#     parser.add_argument("-p", "--checkpoint", type=str, required=True, help="Path to .pth model")
#     # æ–°å¢åƒæ•¸ï¼šåŸå§‹åœ–ç‰‡è·¯å¾‘
#     parser.add_argument("--original", type=str, default=None, help="Path to original image (for PSNR calculation)")
#     parser.add_argument("--cuda", action="store_true", default=True)
#     args = parser.parse_args()
#
#     device = "cuda" if args.cuda and torch.cuda.is_available() else "cpu"
#     PATCH_SIZE = 256
#
#     # 1. æœå°‹ .bin æª”æ¡ˆ
#     bin_files = glob.glob(os.path.join(args.bin_dir, "*.bin"))
#     if not bin_files:
#         print(f"åœ¨ {args.bin_dir} æ‰¾ä¸åˆ°ä»»ä½• .bin æª”æ¡ˆ")
#         sys.exit(1)
#
#     print(f"æ‰¾åˆ° {len(bin_files)} å€‹å£“ç¸®æª”ï¼Œæº–å‚™è§£å£“ç¸®...")
#
#     # 2. åˆ†ææª”å
#     max_row, max_col = 0, 0
#     pattern = re.compile(r"_row(\d+)_col(\d+)\.bin$")
#     valid_files = []
#     base_name = ""
#
#     for f in bin_files:
#         match = pattern.search(f)
#         if match:
#             r, c = int(match.group(1)), int(match.group(2))
#             max_row = max(max_row, r)
#             max_col = max(max_col, c)
#             valid_files.append((r, c, f))
#             if base_name == "":
#                 base_name = os.path.basename(f).replace(match.group(0), "")
#
#     # è¨ˆç®—ç•«å¸ƒå¤§å°
#     canvas_w = (max_col + 1) * PATCH_SIZE
#     canvas_h = (max_row + 1) * PATCH_SIZE
#     print(f"åµæ¸¬åˆ°çŸ©é™£: {max_row + 1}x{max_col + 1} | é‡å»ºç•«å¸ƒ: {canvas_w}x{canvas_h}")
#
#     full_recon_img = Image.new('RGB', (canvas_w, canvas_h))
#
#     # 3. è¼‰å…¥æ¨¡å‹
#     model = load_checkpoint(args.checkpoint).to(device)
#     model.update(force=True)
#
#     # 4. è§£å£“ç¸®ä¸¦æ‹¼è²¼
#     count = 0
#     for r, c, fpath in valid_files:
#         count += 1
#         print(f"è§£å£“ç¸®: {os.path.basename(fpath)} ({count}/{len(valid_files)})", end='\r')
#         x_hat = process_decompress(model, fpath, device)
#         rec_tensor = x_hat.squeeze().cpu().clamp(0, 1)
#         rec_patch_pil = transforms.ToPILImage()(rec_tensor)
#
#         left = c * PATCH_SIZE
#         upper = r * PATCH_SIZE
#         full_recon_img.paste(rec_patch_pil, (left, upper))
#
#     print("\nè§£å£“ç¸®å®Œæˆï¼Œæ­£åœ¨å„²å­˜å¤§åœ–...")
#
#     output_filename = f"{base_name}_RECONSTRUCTED.png"
#     output_path = os.path.join(args.bin_dir, output_filename)
#     full_recon_img.save(output_path)
#     print(f"å®Œæ•´é‚„åŸåœ–å·²å„²å­˜è‡³: {output_path}")
#
#     # ==========================================================================
#     # 5. è¨ˆç®— PSNR (å¦‚æœä½¿ç”¨è€…æœ‰æä¾›åŸå§‹åœ–)
#     # ==========================================================================
#     if args.original:
#         print("-" * 40)
#         print("æ­£åœ¨è¨ˆç®— PSNR...")
#
#         if not os.path.exists(args.original):
#             print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°åŸå§‹åœ–ç‰‡ {args.original}")
#             return
#
#         # è®€å–åŸå§‹åœ–
#         try:
#             # ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„è®€å–é‚è¼¯ (æ­£è¦åŒ–åˆ° 0-1)
#             gt_tensor = read_original_image(args.original)
#
#             # è®€å–å‰›é‡å»ºå¥½çš„åœ– (è½‰ç‚º Tensor 0-1)
#             rec_tensor = transforms.ToTensor()(full_recon_img)
#
#             # ç¢ºä¿å°ºå¯¸ä¸€è‡´ (é‡å°é‚Šç·£å¯èƒ½è¢«è£åˆ‡çš„æƒ…æ³)
#             # å¦‚æœé‡å»ºåœ–æ¯”åŸå§‹åœ–å° (å› ç‚º patch æ²’åˆ‡æ»¿)ï¼Œå‰‡è£åˆ‡åŸå§‹åœ–ä¾†å°é½Š
#             # å¦‚æœé‡å»ºåœ–æ¯”åŸå§‹åœ–å¤§ (å› ç‚º padding)ï¼Œå‰‡è£åˆ‡é‡å»ºåœ–
#             h_gt, w_gt = gt_tensor.shape[1], gt_tensor.shape[2]
#             h_rec, w_rec = rec_tensor.shape[1], rec_tensor.shape[2]
#
#             min_h = min(h_gt, h_rec)
#             min_w = min(w_gt, w_rec)
#
#             gt_tensor = gt_tensor[:, :min_h, :min_w]
#             rec_tensor = rec_tensor[:, :min_h, :min_w]
#
#             # è¨ˆç®—æ•¸å€¼
#             val_psnr = psnr(gt_tensor, rec_tensor)
#             val_msssim = ms_ssim(gt_tensor.unsqueeze(0), rec_tensor.unsqueeze(0), data_range=1.0).item()
#
#             print(f"åŸå§‹åœ–å°ºå¯¸: {w_gt}x{h_gt}")
#             print(f"é‡å»ºåœ–å°ºå¯¸: {w_rec}x{h_rec}")
#             print(f"æ¯”å°å€åŸŸ:   {min_w}x{min_h}")
#             print("-" * 40)
#             print(f"ğŸš€ PSNR:    {val_psnr:.4f} dB")
#             print(f"ğŸš€ MS-SSIM: {val_msssim:.4f}")
#             print("-" * 40)
#
#         except Exception as e:
#             print(f"è¨ˆç®— PSNR æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
#             import traceback
#             traceback.print_exc()
#
#
# if __name__ == "__main__":
#     main()

# import argparse
# import os
# import sys
# import glob
# import math
# import struct
# import zlib
# from collections import OrderedDict

# import torch
# import torch.nn.functional as F
# from torchvision import transforms
# from PIL import Image

# # ==============================================================================
# # è¨­å®šï¼šåŒ¯å…¥æ¨¡å‹
# # ==============================================================================
# try:
#     from conv2 import SimpleConvStudentModel
# except ImportError:
#     print("éŒ¯èª¤: æ‰¾ä¸åˆ° conv2.pyï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚")
#     sys.exit(1)

# try:
#     import rasterio
# except ImportError:
#     rasterio = None

# try:
#     from pytorch_msssim import ms_ssim

#     HAS_MSSSIM = True
# except ImportError:
#     HAS_MSSSIM = False


# # ==============================================================================
# # Monkey Patching: æ³¨å…¥è§£å£“ç¸®æ–¹æ³•
# # ==============================================================================
# def decompress_method(self, strings, shape):
#     assert isinstance(strings, list) and len(strings) == 2
#     z_hat = self.entropy_bottleneck.decompress(strings[1], shape)
#     gaussian_params = self.h_s(z_hat)
#     scales_hat, means_hat = gaussian_params.chunk(2, 1)
#     indexes = self.gaussian_conditional.build_indexes(scales_hat)
#     y_hat = self.gaussian_conditional.decompress(strings[0], indexes, means=means_hat)
#     x_hat = self.g_s(y_hat).clamp_(0, 1)
#     return {"x_hat": x_hat}


# SimpleConvStudentModel.decompress = decompress_method


# # ==============================================================================
# # è¡›æ˜Ÿé€šè¨Šå°ˆç”¨ï¼šå°åŒ…è®€å–èˆ‡é©—è­‰ (æ ¸å¿ƒä¿®æ”¹)
# # ==============================================================================
# def parse_payload_bytes(payload_data):
#     """
#     è§£æ Payload çš„äºŒé€²ä½å…§å®¹ï¼Œé‚„åŸæˆ strings å’Œ shape
#     æ ¼å¼: Shape(2+2) + LenZ(4) + Z + LenY(4) + Y
#     """
#     cursor = 0

#     # è®€å– shape
#     h = int.from_bytes(payload_data[cursor:cursor + 2], 'little')
#     cursor += 2
#     w = int.from_bytes(payload_data[cursor:cursor + 2], 'little')
#     cursor += 2
#     shape = (h, w)

#     # è®€å– z_string
#     len_z = int.from_bytes(payload_data[cursor:cursor + 4], 'little')
#     cursor += 4
#     z_str = payload_data[cursor:cursor + len_z]
#     cursor += len_z

#     # è®€å– y_string
#     len_y = int.from_bytes(payload_data[cursor:cursor + 4], 'little')
#     cursor += 4
#     y_str = payload_data[cursor:cursor + len_y]


# @torch.no_grad()
# def process_decompress_packet(model, packet_data):
#     out_dec = model.decompress(packet_data["strings"], packet_data["shape"])
#     x_hat = out_dec["x_hat"]

#     # è™•ç† Patch å¤§å° (é è¨­ 256)
#     target_h, target_w = 256, 256
#     curr_h, curr_w = x_hat.size(2), x_hat.size(3)

#     if curr_h != target_h or curr_w != target_w:
#         padding_left = (curr_w - target_w) // 2
#         padding_top = (curr_h - target_h) // 2
#         x_hat = x_hat[:, :, padding_top:padding_top + target_h, padding_left:padding_left + target_w]

#     return x_hat


# def psnr(a: torch.Tensor, b: torch.Tensor) -> float:
#     mse = F.mse_loss(a, b).item()
#     return -10 * math.log10(mse) if mse > 0 else float('inf')


# def read_original_image(filepath: str) -> torch.Tensor:
#     ext = os.path.splitext(filepath)[-1].lower()
#     if ext in ['.tif', '.tiff']:
#         if rasterio is None: raise RuntimeError("éœ€å®‰è£ rasterio æ‰èƒ½è®€å– TIF")
#         SCALE = 10000.0
#         with rasterio.open(filepath) as src:
#             raw_data = src.read().astype(np.float32)
#         if np.isnan(raw_data).any(): raw_data = np.nan_to_num(raw_data)
#         rgb_data = raw_data[:3, :, :] if raw_data.shape[0] >= 3 else raw_data
#         clipped_data = np.clip(rgb_data, 0.0, 10000.0)
#         return torch.from_numpy(clipped_data / SCALE)
#     else:
#         img = Image.open(filepath).convert("RGB")
#         return transforms.ToTensor()(img)


# def load_checkpoint(checkpoint_path):
#     print(f"Loading checkpoint: {checkpoint_path}")
#     checkpoint = torch.load(checkpoint_path, map_location='cpu')
#     state_dict = checkpoint.get("state_dict", checkpoint)
#     new_state_dict = OrderedDict()
#     for k, v in state_dict.items():
#         name = k[7:] if k.startswith('module.') else k
#         new_state_dict[name] = v

#     N, M = 128, 192
#     try:
#         N = new_state_dict['g_a.0.weight'].size(0)
#         keys = sorted([k for k in new_state_dict.keys() if 'g_a' in k and 'weight' in k])
#         M = new_state_dict[keys[-1]].size(0)
#     except:
#         pass

#     model = SimpleConvStudentModel(N=N, M=M)
#     model.load_state_dict(new_state_dict, strict=False)
#     return model.eval()


# # ==============================================================================
# # ä¸»ç¨‹å¼
# # ==============================================================================
# def main():
#     parser = argparse.ArgumentParser(description="Satellite Image Decompression (CRC32 Verified)")
#     parser.add_argument("bin_dir", type=str, help="Directory containing .bin files")
#     parser.add_argument("-p", "--checkpoint", type=str, required=True, help="Path to .pth model")
#     parser.add_argument("--original", type=str, default=None, help="Original image for PSNR")
#     parser.add_argument("--cuda", action="store_true", default=True)
#     parser.add_argument("--target_id", type=int, default=None, help="Only reconstruct packets with this Image ID")
#     args = parser.parse_args()

#     device = "cuda" if args.cuda and torch.cuda.is_available() else "cpu"
#     PATCH_SIZE = 256

#     # 1. æœå°‹æ‰€æœ‰ .bin æª”æ¡ˆ (ä¸å†ä¾è³´æª”åæ ¼å¼)
#     bin_files = glob.glob(os.path.join(args.bin_dir, "*.bin"))
#     if not bin_files:
#         print(f"åœ¨ {args.bin_dir} æ‰¾ä¸åˆ°ä»»ä½• .bin æª”æ¡ˆ")
#         sys.exit(1)

#     print(f"æ‰¾åˆ° {len(bin_files)} å€‹æª”æ¡ˆï¼Œé–‹å§‹é©—è­‰ä¸¦è§£å£“ç¸®...")

#     # 2. è¼‰å…¥æ¨¡å‹
#     model = load_checkpoint(args.checkpoint).to(device)
#     model.update(force=True)

#     # 3. é æƒæï¼šæ±ºå®šç•«å¸ƒå¤§å°
#     # å› ç‚ºæ²’æœ‰æª”åå‘Šè¨´æˆ‘å€‘å¤§å°ï¼Œæˆ‘å€‘å¿…é ˆæƒææœ‰æ•ˆå°åŒ…çš„ header ä¾†æ‰¾å‡ºæœ€å¤§çš„ row/col
#     max_row, max_col = 0, 0
#     valid_packets = []

#     for f in bin_files:
#         # è®€å–ä¸¦é©—è­‰å°åŒ…
#         packet = load_satellite_packet(f)

#         if packet is None:
#             # é©—è­‰å¤±æ•—ï¼Œç›´æ¥è·³é (é€™å°±æ˜¯æ‰åŒ…/å£åŒ…è™•ç†)
#             continue

#         # å¦‚æœæŒ‡å®šäº† IDï¼Œéæ¿¾æ‰ä¸ç¬¦çš„
#         if args.target_id is not None and packet['img_id'] != args.target_id:
#             continue

#         max_row = max(max_row, packet['row'])
#         max_col = max(max_col, packet['col'])
#         valid_packets.append(packet)

#     if not valid_packets:
#         print("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å°åŒ… (å¯èƒ½å…¨éƒ¨ææ¯€æˆ– ID ä¸ç¬¦)ã€‚")
#         sys.exit(1)

#     # è¨ˆç®—ç•«å¸ƒå¤§å°
#     canvas_w = (max_col + 1) * PATCH_SIZE
#     canvas_h = (max_row + 1) * PATCH_SIZE
#     print(f"æœ€å¤§çŸ©é™£: Row {max_row}, Col {max_col} | é‡å»ºç•«å¸ƒ: {canvas_w}x{canvas_h}")
#     print(f"æœ‰æ•ˆå°åŒ…æ•¸: {len(valid_packets)} / {len(bin_files)} (éºå¤±/ææ¯€: {len(bin_files) - len(valid_packets)})")

#     # å»ºç«‹é»‘è‰²ç•«å¸ƒ (RGB 0,0,0)
#     full_recon_img = Image.new('RGB', (canvas_w, canvas_h), (0, 0, 0))

#     # 4. è§£å£“ç¸®ä¸¦æ‹¼è²¼
#     count = 0
#     for packet in valid_packets:
#         count += 1
#         r, c = packet['row'], packet['col']
#         print(f"é‚„åŸå€å¡Š: ({r}, {c}) - ID:{packet['img_id']} ({count}/{len(valid_packets)})", end='\r')

#         try:
#             x_hat = process_decompress_packet(model, packet)
#             rec_tensor = x_hat.squeeze().cpu().clamp(0, 1)
#             rec_patch_pil = transforms.ToPILImage()(rec_tensor)

#             left = c * PATCH_SIZE
#             upper = r * PATCH_SIZE
#             full_recon_img.paste(rec_patch_pil, (left, upper))
#         except Exception as e:
#             print(f"\n[è§£ç¢¼å¤±æ•—] å€å¡Š ({r},{c}) è³‡æ–™ç•°å¸¸: {e}")

#     print("\næ‹¼è²¼å®Œæˆï¼Œå„²å­˜å½±åƒ...")
#     output_path = os.path.join(args.bin_dir, "RECONSTRUCTED_SATELLITE.png")
#     full_recon_img.save(output_path)
#     print(f"çµæœå·²å„²å­˜: {output_path}")

#     # ==========================================================================
#     # 5. è¨ˆç®— PSNR
#     # ==========================================================================
#     if args.original:
#         print("-" * 40)
#         if not os.path.exists(args.original):
#             print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°åŸå§‹åœ–ç‰‡ {args.original}")
#             return

#         try:
#             gt_tensor = read_original_image(args.original)
#             rec_tensor = transforms.ToTensor()(full_recon_img)

#             # å°é½Šå°ºå¯¸
#             h_gt, w_gt = gt_tensor.shape[1], gt_tensor.shape[2]
#             h_rec, w_rec = rec_tensor.shape[1], rec_tensor.shape[2]
#             min_h = min(h_gt, h_rec)
#             min_w = min(w_gt, w_rec)

#             gt_tensor = gt_tensor[:, :min_h, :min_w]
#             rec_tensor = rec_tensor[:, :min_h, :min_w]

#             val_psnr = psnr(gt_tensor, rec_tensor)
#             val_msssim = 0.0
#             if HAS_MSSSIM:
#                 val_msssim = ms_ssim(gt_tensor.unsqueeze(0), rec_tensor.unsqueeze(0), data_range=1.0).item()

#             print(f"ğŸš€ PSNR:    {val_psnr:.4f} dB")
#             if HAS_MSSSIM:
#                 print(f"ğŸš€ MS-SSIM: {val_msssim:.4f}")
#             print("-" * 40)

#         except Exception as e:
#             print(f"è¨ˆç®— PSNR æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


# if __name__ == "__main__":
#     main()

import argparse
import os
import sys
import glob
import math
import struct
import zlib
from collections import OrderedDict

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image

# ==============================================================================
# è¨­å®šï¼šåŒ¯å…¥æ¨¡å‹
# ==============================================================================
try:
    from conv2 import SimpleConvStudentModel
except ImportError as e:
    print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° conv2.pyï¼Œæˆ–å…¶ä¾è³´å¥—ä»¶è¼‰å…¥å¤±æ•—ã€‚\nè©³ç´°éŒ¯èª¤: {e}")
    sys.exit(1)

try:
    import rasterio
except ImportError:
    rasterio = None

try:
    from pytorch_msssim import ms_ssim

    HAS_MSSSIM = True
except ImportError:
    HAS_MSSSIM = False


# ==============================================================================
# Monkey Patching: æ³¨å…¥è§£å£“ç¸®æ–¹æ³• (Hybrid CPU/GPU Mode)
# ==============================================================================
def decompress_method(self, strings, shape):
    assert isinstance(strings, list) and len(strings) == 2
    
    # Force CPU for all critical components
    self.entropy_bottleneck.cpu()
    self.h_s.cpu()
    self.gaussian_conditional.cpu()
    
    z_hat = self.entropy_bottleneck.decompress(strings[1], shape)
    if z_hat.device.type != 'cpu':
        z_hat = z_hat.cpu()
    
    # [DEBUG] Only print once
    if not hasattr(self, '_eb_debug_printed') or not self._eb_debug_printed:
        eb = self.entropy_bottleneck
        medians = eb.quantiles[:, 0, 1]
        print(f"[DEC] eb._quantized_cdf sum: {eb._quantized_cdf.sum().item()}")
        print(f"[DEC] eb._offset sum: {eb._offset.sum().item()}")
        print(f"[DEC] medians sum: {medians.sum().item():.6f}")
        print(f"[DEC] z_strings[0] len: {len(strings[1][0])}")
        self._eb_debug_printed = True
        
    gaussian_params = self.h_s(z_hat)
    scales_hat_raw, means_hat_raw = gaussian_params.chunk(2, 1)

    # [V11] Cross-Platform Deterministic Quantization
    scales_clamped = scales_hat_raw.clamp(0.5, 32.0)
    scale_indices = torch.floor((scales_clamped - 0.5) * 2 + 0.5).to(torch.int64).clamp(0, 63)
    scales_hat = 0.5 + scale_indices.float() * 0.5
    means_hat = torch.floor(means_hat_raw * 10 + 0.5) / 10
    
    indexes = self.gaussian_conditional.build_indexes(scales_hat)
    indexes = indexes.to(dtype=torch.int32).contiguous()
    means_hat = means_hat.contiguous()
    
    # [DEBUG] Comprehensive checksums (only first patch)
    if not hasattr(self, '_debug_printed') or not self._debug_printed:
        print(f"[DEC] z_hat sum: {z_hat.sum().item():.6f}")
        print(f"[DEC] scales_hat_raw sum: {scales_hat_raw.sum().item():.6f}")
        print(f"[DEC] scale_indices sum: {scale_indices.sum().item()}")
        print(f"[DEC] means_hat sum: {means_hat.sum().item():.6f}")
        print(f"[DEC] indexes sum: {indexes.sum().item()}")
        print(f"[DEC] gc._quantized_cdf[0,:5]: {self.gaussian_conditional._quantized_cdf[0,:5].tolist()}")
        self._debug_printed = True
    
    if indexes.max().item() >= 64 or indexes.min().item() < 0:
        indexes = indexes.clamp(0, 63)
        
    # Decompress Y on CPU
    y_hat = None
    try:
        prev_threads = torch.get_num_threads()
        torch.set_num_threads(1)
        y_hat = self.gaussian_conditional.decompress(strings[0], indexes.cpu(), means=means_hat.cpu())
        torch.set_num_threads(prev_threads)
    except Exception as e:
        print(f"[ERROR] Decompression Failed: {e}")
        y_hat = torch.zeros_like(means_hat)

    # ç¢ºä¿ç„¡ NaN
    if torch.isnan(y_hat).any():
        print("!! WARNING !! NaN detected in y_hat. Replacing with zeros.")
        y_hat = torch.nan_to_num(y_hat)
        
    # 2. Main Transform (GPU)
    # å°‡è§£ç¢¼å®Œçš„ Y ç§»å› GPU ä»¥é€²è¡Œ g_s
    
    # å–å¾— GPU device from g_s parameters
    device = next(self.g_s.parameters()).device
    y_hat = y_hat.to(device)
    
    x_hat = self.g_s(y_hat).clamp_(0, 1)
    return {"x_hat": x_hat}


SimpleConvStudentModel.decompress = decompress_method


def parse_payload_bytes(payload_data):
    """
    è§£æ Payload çš„äºŒé€²ä½å…§å®¹ï¼Œé‚„åŸæˆ strings å’Œ shape
    æ ¼å¼: Shape(2+2) + LenZ(4) + Z + LenY(4) + Y
    """
    cursor = 0

    # è®€å– shape
    h = int.from_bytes(payload_data[cursor:cursor + 2], 'little')
    cursor += 2
    w = int.from_bytes(payload_data[cursor:cursor + 2], 'little')
    cursor += 2
    shape = (h, w)

    # è®€å– z_string
    len_z = int.from_bytes(payload_data[cursor:cursor + 4], 'little')
    cursor += 4
    z_str = payload_data[cursor:cursor + len_z]
    cursor += len_z

    # è®€å– y_string
    len_y = int.from_bytes(payload_data[cursor:cursor + 4], 'little')
    cursor += 4
    y_str = payload_data[cursor:cursor + len_y]

    return {"strings": [[y_str], [z_str]], "shape": shape}


from conv2 import get_scale_table

def load_satellite_packet(bin_path):
    """
    è®€å–å°åŒ…: Header + Y_Strings + Z_Strings (AI Compressed) + CRC32
    Format (Little Endian <):
    [Magic:3][ID:1][Row:1][Col:1][H:2][W:2][LenY:4][LenZ:4] ... [CRC:4]
    """
    import numpy as np
    try:
        with open(bin_path, "rb") as f:
            data = f.read()

        # æœ€å°é•·åº¦æª¢æŸ¥: Header(18) + CRC(4) = 22 bytes
        if len(data) < 22: 
            print(f"[Error] File too small: {os.path.basename(bin_path)}")
            return None

        # 1. CRC Check (æœ€å¾Œ 4 bytes)
        received_crc = struct.unpack('<I', data[-4:])[0]
        content = data[:-4]
        calc_crc = zlib.crc32(content) & 0xffffffff

        if received_crc != calc_crc:
            print(f"[Corrupt] CRC Fail: {os.path.basename(bin_path)}")
            return None

        # 2. Parse Header (18 bytes)
        # Magic(3) + ID(1) + Row(1) + Col(1) + H(2) + W(2) + LenY(4) + LenZ(4)
        header_size = 18
        header_data = data[:header_size]
        
        magic, img_id, row, col, h, w, len_y, len_z = struct.unpack('<3sBBBHHII', header_data)
        
        # Magic Check
        if magic != b'TIC':
            print(f"[Error] Invalid Magic: {magic}")
            return None
            
        # 3. Parse Payloads
        # Payload starts at 18
        cursor = header_size
        
        if len(data) < cursor + len_y + len_z + 4:
            print(f"[Error] Incomplete Payload")
            return None
            
        y_str = data[cursor : cursor + len_y]
        cursor += len_y
        
        z_str = data[cursor : cursor + len_z]
        
        # å›å‚³åŸå§‹å­—ä¸²ï¼Œè®“ decompress_method å»è§£ç¢¼
        return {
            "row": row, "col": col, "img_id": img_id,
            "strings": [[y_str], [z_str]], 
            "shape": (h, w)
        }

    except Exception as e:
        print(f"[Read Error] {bin_path}: {e}")
        return None


@torch.no_grad()
def process_decompress_packet(model, packet_data):
    out_dec = model.decompress(packet_data["strings"], packet_data["shape"])
    x_hat = out_dec["x_hat"]

    # è™•ç† Patch å¤§å° (é è¨­ 256)
    target_h, target_w = 256, 256
    curr_h, curr_w = x_hat.size(2), x_hat.size(3)

    if curr_h != target_h or curr_w != target_w:
        padding_left = (curr_w - target_w) // 2
        padding_top = (curr_h - target_h) // 2
        x_hat = x_hat[:, :, padding_top:padding_top + target_h, padding_left:padding_left + target_w]

    return x_hat


def psnr(a: torch.Tensor, b: torch.Tensor) -> float:
    mse = F.mse_loss(a, b).item()
    return -10 * math.log10(mse) if mse > 0 else float('inf')


def read_original_image(filepath: str) -> torch.Tensor:
    ext = os.path.splitext(filepath)[-1].lower()
    if ext in ['.tif', '.tiff']:
        if rasterio is None: raise RuntimeError("éœ€å®‰è£ rasterio æ‰èƒ½è®€å– TIF")
        SCALE = 10000.0
        with rasterio.open(filepath) as src:
            raw_data = src.read().astype(np.float32)
        if np.isnan(raw_data).any(): raw_data = np.nan_to_num(raw_data)
        rgb_data = raw_data[:3, :, :] if raw_data.shape[0] >= 3 else raw_data
        clipped_data = np.clip(rgb_data, 0.0, 10000.0)
        return torch.from_numpy(clipped_data / SCALE)
    else:
        img = Image.open(filepath).convert("RGB")
        return transforms.ToTensor()(img)


def load_checkpoint(checkpoint_path):
    print(f"Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint.get("state_dict", checkpoint)
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[7:] if k.startswith('module.') else k
        new_state_dict[name] = v

    N, M = 128, 192
    try:
        N = new_state_dict['g_a.0.weight'].size(0)
        keys = sorted([k for k in new_state_dict.keys() if 'g_a' in k and 'weight' in k])
        M = new_state_dict[keys[-1]].size(0)
    except:
        pass
    
    # DEBUG: Print keys related to gaussian_conditional
    print("\n[DEBUG] Checkpoint Keys (GaussianConditional):")
    for k in new_state_dict.keys():
        if "gaussian_conditional" in k:
            # print(f"{k}: {new_state_dict[k].shape if hasattr(new_state_dict[k], 'shape') else 'No Shape'}")
            pass

    model = SimpleConvStudentModel(N=N, M=M)
    model.load_state_dict(new_state_dict, strict=True)
    
    # ==========================================================================
    # é‡åŒ–ç­–ç•¥: å¼·åˆ¶çµ±ä¸€ Scale Table (Coarse Grid)
    # ==========================================================================
    # ä½¿ç”¨ç²—åˆ»åº¦ 0.5 ~ 32.0 (å…± 64 éš)
    scale_table = torch.linspace(0.5, 32.0, 64)
    
    # å¼·åˆ¶æ›´æ–°æ¨¡å‹å…§çš„è¡¨å’Œ CDF
    model.gaussian_conditional.update_scale_table(scale_table, force=True)
    model.update(force=True)
    
    # ==========================================================================
    # å¼·åˆ¶çµ±ä¸€ EntropyBottleneck & GaussianConditional CDFs
    # ==========================================================================
    try:
        from fixed_cdfs import FIXED_EB_CDF, FIXED_EB_OFFSET, FIXED_EB_LENGTH, FIXED_EB_MEDIANS
        from fixed_cdfs import FIXED_GC_CDF, FIXED_GC_OFFSET, FIXED_GC_LENGTH, FIXED_GC_SCALE_TABLE
        
        eb = model.entropy_bottleneck
        gc = model.gaussian_conditional
        device = eb._quantized_cdf.device
        
        # EntropyBottleneck CDFs
        eb._quantized_cdf.resize_(torch.tensor(FIXED_EB_CDF).shape).copy_(
            torch.tensor(FIXED_EB_CDF, device=device, dtype=torch.int32))
        eb._offset.resize_(torch.tensor(FIXED_EB_OFFSET).shape).copy_(
            torch.tensor(FIXED_EB_OFFSET, device=device, dtype=torch.int32))
        eb._cdf_length.resize_(torch.tensor(FIXED_EB_LENGTH).shape).copy_(
            torch.tensor(FIXED_EB_LENGTH, device=device, dtype=torch.int32))
        fixed_medians = torch.tensor(FIXED_EB_MEDIANS, device=device)
        eb.quantiles.data[:, 0, 1] = fixed_medians.squeeze()
        
        # GaussianConditional CDFs
        gc._quantized_cdf.resize_(torch.tensor(FIXED_GC_CDF).shape).copy_(
            torch.tensor(FIXED_GC_CDF, device=device, dtype=torch.int32))
        gc._offset.resize_(torch.tensor(FIXED_GC_OFFSET).shape).copy_(
            torch.tensor(FIXED_GC_OFFSET, device=device, dtype=torch.int32))
        gc._cdf_length.resize_(torch.tensor(FIXED_GC_LENGTH).shape).copy_(
            torch.tensor(FIXED_GC_LENGTH, device=device, dtype=torch.int32))
        gc.scale_table = torch.tensor(FIXED_GC_SCALE_TABLE, device=device)
            
        print("[INFO] EntropyBottleneck & GaussianConditional CDFs overwritten.")
    except ImportError:
        print("[WARNING] fixed_cdfs.py not found or incomplete!")
    except Exception as e:
        print(f"[WARNING] Failed to overwrite CDFs: {e}")
    # ==========================================================================

    return model.eval()

    return model.eval()


# ==============================================================================
# ä¸»ç¨‹å¼
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="Satellite Image Decompression (CRC32 Verified)")
    parser.add_argument("bin_dir", type=str, help="Directory containing .bin files")
    parser.add_argument("-p", "--checkpoint", type=str, required=True, help="Path to .pth model")
    parser.add_argument("--original", type=str, default=None, help="Original image for PSNR")
    parser.add_argument("--cuda", action="store_true", default=True)
    parser.add_argument("--target_id", type=int, default=None, help="Only reconstruct packets with this Image ID")
    args = parser.parse_args()

    device = "cuda" if args.cuda and torch.cuda.is_available() else "cpu"
    PATCH_SIZE = 256

    # 1. æœå°‹æ‰€æœ‰ .bin æª”æ¡ˆ (ä¸å†ä¾è³´æª”åæ ¼å¼)
    bin_files = glob.glob(os.path.join(args.bin_dir, "*.bin"))
    if not bin_files:
        print(f"åœ¨ {args.bin_dir} æ‰¾ä¸åˆ°ä»»ä½• .bin æª”æ¡ˆ")
        sys.exit(1)

    print(f"æ‰¾åˆ° {len(bin_files)} å€‹æª”æ¡ˆï¼Œé–‹å§‹é©—è­‰ä¸¦è§£å£“ç¸®...")

    # 2. è¼‰å…¥æ¨¡å‹
    model = load_checkpoint(args.checkpoint).to(device)
    # [CRITICAL] DO NOT call model.update() here! It will overwrite the fixed_cdfs loaded in load_checkpoint
    # model.update(force=True)  # REMOVED - This was causing z_hat mismatch across platforms

    # 3. é æƒæï¼šæ±ºå®šç•«å¸ƒå¤§å°
    # å› ç‚ºæ²’æœ‰æª”åå‘Šè¨´æˆ‘å€‘å¤§å°ï¼Œæˆ‘å€‘å¿…é ˆæƒææœ‰æ•ˆå°åŒ…çš„ header ä¾†æ‰¾å‡ºæœ€å¤§çš„ row/col
    max_row, max_col = 0, 0
    valid_packets = []

    for f in bin_files:
        # è®€å–ä¸¦é©—è­‰å°åŒ…
        packet = load_satellite_packet(f)

        if packet is None:
            # é©—è­‰å¤±æ•—ï¼Œç›´æ¥è·³é (é€™å°±æ˜¯æ‰åŒ…/å£åŒ…è™•ç†)
            continue

        # å¦‚æœæŒ‡å®šäº† IDï¼Œéæ¿¾æ‰ä¸ç¬¦çš„
        if args.target_id is not None and packet['img_id'] != args.target_id:
            continue

        max_row = max(max_row, packet['row'])
        max_col = max(max_col, packet['col'])
        valid_packets.append(packet)

    if not valid_packets:
        print("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å°åŒ… (å¯èƒ½å…¨éƒ¨ææ¯€æˆ– ID ä¸ç¬¦)ã€‚")
        sys.exit(1)

    # è¨ˆç®—ç•«å¸ƒå¤§å°
    canvas_w = (max_col + 1) * PATCH_SIZE
    canvas_h = (max_row + 1) * PATCH_SIZE
    print(f"æœ€å¤§çŸ©é™£: Row {max_row}, Col {max_col} | é‡å»ºç•«å¸ƒ: {canvas_w}x{canvas_h}")
    print(f"æœ‰æ•ˆå°åŒ…æ•¸: {len(valid_packets)} / {len(bin_files)} (éºå¤±/ææ¯€: {len(bin_files) - len(valid_packets)})")

    # å»ºç«‹é»‘è‰²ç•«å¸ƒ (RGB 0,0,0)
    full_recon_img = Image.new('RGB', (canvas_w, canvas_h), (0, 0, 0))

    # 4. è§£å£“ç¸®ä¸¦æ‹¼è²¼
    count = 0
    for packet in valid_packets:
        count += 1
        r, c = packet['row'], packet['col']
        print(f"é‚„åŸå€å¡Š: ({r}, {c}) - ID:{packet['img_id']} ({count}/{len(valid_packets)})", end='\r')

        try:
            x_hat = process_decompress_packet(model, packet)
            rec_tensor = x_hat.squeeze().cpu().clamp(0, 1)
            rec_patch_pil = transforms.ToPILImage()(rec_tensor)

            left = c * PATCH_SIZE
            upper = r * PATCH_SIZE
            full_recon_img.paste(rec_patch_pil, (left, upper))
        except Exception as e:
            print(f"\n[è§£ç¢¼å¤±æ•—] å€å¡Š ({r},{c}) è³‡æ–™ç•°å¸¸: {e}")

    print("\næ‹¼è²¼å®Œæˆï¼Œå„²å­˜å½±åƒ...")
    output_path = os.path.join(args.bin_dir, "RECONSTRUCTED_SATELLITE.png")
    full_recon_img.save(output_path)
    print(f"çµæœå·²å„²å­˜: {output_path}")

    # ==========================================================================
    # 5. è¨ˆç®— PSNR
    # ==========================================================================
    if args.original:
        print("-" * 40)
        if not os.path.exists(args.original):
            print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°åŸå§‹åœ–ç‰‡ {args.original}")
            return

        try:
            gt_tensor = read_original_image(args.original)
            rec_tensor = transforms.ToTensor()(full_recon_img)

            # å°é½Šå°ºå¯¸
            h_gt, w_gt = gt_tensor.shape[1], gt_tensor.shape[2]
            h_rec, w_rec = rec_tensor.shape[1], rec_tensor.shape[2]
            min_h = min(h_gt, h_rec)
            min_w = min(w_gt, w_rec)

            gt_tensor = gt_tensor[:, :min_h, :min_w]
            rec_tensor = rec_tensor[:, :min_h, :min_w]

            val_psnr = psnr(gt_tensor, rec_tensor)
            val_msssim = 0.0
            if HAS_MSSSIM:
                val_msssim = ms_ssim(gt_tensor.unsqueeze(0), rec_tensor.unsqueeze(0), data_range=1.0).item()

            print(f"ğŸš€ PSNR:    {val_psnr:.4f} dB")
            if HAS_MSSSIM:
                print(f"ğŸš€ MS-SSIM: {val_msssim:.4f}")
            print("-" * 40)

        except Exception as e:
            print(f"è¨ˆç®— PSNR æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()